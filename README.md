# DeepLearning.AI: Generative AI with Large Language Models

Lab reports and findings from the DeepLearning.AI course on LLM applications.

## About

I'm documenting my learning journey through this course. The platform doesn't allow code sharing, so I'm publishing my experimental findings and analysis instead.

## Course Structure

- **Week 1:** Transformer architecture, prompt engineering, generative configuration
- **Week 2:** Fine-tuning and evaluation (in progress)
- **Week 3:** Reinforcement learning and LLM-powered applications (upcoming)

## Lab Reports

### Week 1: Dialogue Summarization Experiments

[Full report â†’](./week1-lab-report-prompt-engineering-experiments.md)

Key findings:
- Zero-shot, one-shot, and few-shot prompting all struggled with speaker identification
- Temperature settings affected creativity but didn't fix core comprehension issues
- Configuration parameters (top_k, top_p, temperature) showed limited impact compared to fine-tuning needs

Main takeaway: Prompt engineering alone isn't enough for complex dialogue tasks. Fine-tuning or chain-of-thought prompting needed.

## Background

This is part of my career transition from retail operations to AI engineering. Building practical LLM applications while working full-time.

Timeline: Nov 2025 â†’ Sep 2026

## Connect

- **LinkedIn:** [linkedin.com/in/chkechagias](https://www.linkedin.com/in/chkechagias)
- **GitHub:** [github.com/chris-kechagias](https://github.com/chris-kechagias)
- **Email:** ck.chris.kechagias@gmail.com
- **Location:** Thessaloniki, Greece ðŸ‡¬ðŸ‡·
- **Open to:** Remote AI Engineer positions (EU/Worldwide)
- **Portfolio:** [In progress]

---

Course by DeepLearning.AI 
